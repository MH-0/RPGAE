#  Exploring the Representational Power of Graph Autoencoders
 Network representation learning has proven to be an effective method for performing various learning tasks in the domain of graphs. This approach aims to project the graph into an embedding space that captures both its structure and content. While representation learning has yielded a great success on many graph learning tasks, there is little understanding behind the structures that are being captured by these embeddings. For example, we wonder if the topological features, such as the degree of the vertex, its local clustering score, and other centrality measures are concretely encoded in the embeddings. Furthermore, we ask if the presence of these structures in the embeddings is necessary for a better performance on the downstream tasks such as clustering and classification. To address these questions, we conduct an extensive empirical study over three classes of unsupervised graph embedding models with a special emphasis on Graph Autoencoders. Subsequently, we find that the topological structures, especially the vertex degree, are indeed captured in the first layer of the Graph Autoencoders that employs the SUM aggregation rule, under the condition that the model preserves the first order proximity. We supplement further evidence for the presence of these features by revealing a hierarchy in the distribution of the vertex degree in the embeddings of the aforementioned model. Finally, we show that the presence of these features does generally improve the performance on some tasks, especially when the ground-truth is correlated with the degree of the vertex.
